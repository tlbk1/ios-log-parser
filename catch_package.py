import os
import re
import json
import zipfile
import requests
from io import BytesIO

def download_file(url, save_path):
    """ä¸‹è½½ZIPæ–‡ä»¶ï¼ˆå¢å¼ºé”™è¯¯å¤„ç†å’Œè¿›åº¦æç¤ºï¼‰"""
    try:
        print(f"å¼€å§‹ä¸‹è½½æ–‡ä»¶: {url}")
        response = requests.get(url, stream=True, timeout=15)
        response.raise_for_status()
        
        # æ˜¾ç¤ºä¸‹è½½è¿›åº¦
        total_size = int(response.headers.get('content-length', 0))
        downloaded = 0
        chunk_size = 8192
        
        with open(save_path, 'wb') as f:
            for chunk in response.iter_content(chunk_size):
                if chunk:
                    f.write(chunk)
                    downloaded += len(chunk)
                    progress = int(100 * downloaded / total_size)
                    print(f"ä¸‹è½½è¿›åº¦: {progress}%", end="\r")
        
        print(f"\nâœ… æ–‡ä»¶ä¸‹è½½æˆåŠŸï¼Œä¿å­˜è‡³: {save_path}")
        return save_path
    except requests.Timeout:
        print("\nâŒ ä¸‹è½½è¶…æ—¶ï¼Œè¯·æ£€æŸ¥ç½‘ç»œè¿æ¥")
    except requests.HTTPError as e:
        print(f"\nâŒ HTTPé”™è¯¯ {e.response.status_code}")
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {str(e)}")
    return None

def extract_zip(zip_path, extract_dir):
    """è§£å‹ZIPæ–‡ä»¶ï¼ˆæ”¯æŒæŒ‡å®šæ–‡ä»¶å¤¹ç»“æ„ï¼‰"""
    try:
        print(f"å¼€å§‹è§£å‹æ–‡ä»¶: {zip_path}")
        with zipfile.ZipFile(zip_path, 'r') as zip_ref:
            # ç¡®ä¿ç›®æ ‡ç›®å½•å­˜åœ¨
            if not os.path.exists(extract_dir):
                os.makedirs(extract_dir)
            
            # è§£å‹æ‰€æœ‰æ–‡ä»¶
            zip_ref.extractall(extract_dir)
        
        print(f"âœ… æ–‡ä»¶è§£å‹æˆåŠŸï¼Œç›®å½•: {extract_dir}")
        return extract_dir
    except zipfile.BadZipFile:
        print(f"âŒ æ— æ•ˆçš„ZIPæ–‡ä»¶: {zip_path}")
    except Exception as e:
        print(f"âŒ è§£å‹å¤±è´¥: {str(e)}")
    return None

def extract_network_info(txt_file):
    """ä»TXTæ–‡ä»¶ä¸­æå–ç½‘ç»œè¯·æ±‚/å“åº”ä¿¡æ¯"""
    network_data = []
    try:
        with open(txt_file, 'r', encoding='utf-8', errors='ignore') as f:
            content = f.read()
        
        # å®šä¹‰ç²¾å‡†çš„ç½‘ç»œä¿¡æ¯æå–è§„åˆ™
        network_patterns = [
            # åŒ¹é…[YFLNetwork]ç›¸å…³çš„ç½‘ç»œè¯·æ±‚
            r'\[YFLNetwork\](.*?)\n',
            # åŒ¹é…å®Œæ•´çš„URL
            r'https?://[^\s"\']+',
            # åŒ¹é…HTTPçŠ¶æ€ç 
            r'\s(200|301|302|400|401|403|404|500)\s',
            # åŒ¹é…JSONæ ¼å¼çš„å“åº”æ•°æ®
            r'\{.*?\}'
        ]
        
        # æå–URLå’ŒçŠ¶æ€ç 
        urls = re.findall(network_patterns[1], content)
        status_codes = re.findall(network_patterns[2], content)
        
        # æå–[YFLNetwork]æ¨¡å—çš„è¯¦ç»†ä¿¡æ¯
        yfl_data = re.findall(network_patterns[0], content, re.DOTALL)
        json_data = []
        
        for item in yfl_data:
            # æå–JSONæ•°æ®
            json_matches = re.findall(network_patterns[3], item)
            for json_str in json_matches:
                try:
                    json_obj = json.loads(json_str)
                    json_data.append(json_obj)
                except:
                    continue
        
        # æ•´ç†æ•°æ®ç»“æ„
        if urls or status_codes or json_data:
            network_data.append({
                'file': os.path.basename(txt_file),
                'urls': list(set(urls)),  # å»é‡åé™åˆ¶æ•°é‡
                'status_codes': list(set(status_codes)),
                'yfl_network': json_data  # æœ€å¤šæå–5æ¡è¯¦ç»†æ•°æ®
            })
    
    except FileNotFoundError:
        print(f"âŒ æ‰¾ä¸åˆ°æ–‡ä»¶: {txt_file}")
    except Exception as e:
        print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {txt_file}: {str(e)}")
    
    return network_data

def process_extracted_files(extract_dir):
    """å¤„ç†è§£å‹åçš„æ–‡ä»¶ï¼Œæå–TXTä¸­çš„ç½‘ç»œä¿¡æ¯"""
    network_data = []
    txt_files = []
    
    # æŸ¥æ‰¾æ‰€æœ‰TXTæ–‡ä»¶ï¼ˆæ”¯æŒå¤šçº§ç›®å½•ï¼‰
    for root, _, files in os.walk(extract_dir):
        for file in files:
            if file.endswith('.txt'):
                txt_files.append(os.path.join(root, file))
    
    if not txt_files:
        print("âŒ æœªæ‰¾åˆ°TXTæ–‡ä»¶")
        return network_data
    
    print(f"æ‰¾åˆ° {len(txt_files)} ä¸ªTXTæ–‡ä»¶ï¼Œå¼€å§‹æå–ç½‘ç»œä¿¡æ¯...")
    
    # å¤„ç†æ¯ä¸ªTXTæ–‡ä»¶
    for txt_file in txt_files:
        file_data = extract_network_info(txt_file)
        if file_data:
            network_data.extend(file_data)
    
    return network_data

def main():
    """ä¸»å‡½æ•°ï¼ˆä¼˜åŒ–å·¥ä½œæµç¨‹ï¼‰"""
    print("=== ç½‘ç»œæ—¥å¿—æå–å·¥å…· ===")
    
    # è·å–ZIPæ–‡ä»¶URL
    log_url = input("è¯·è¾“å…¥ZIPæ–‡ä»¶ä¸‹è½½URL: ").strip()
    if not log_url:
        print("âŒ URLä¸èƒ½ä¸ºç©º")
        return
    
    # å®šä¹‰æ–‡ä»¶è·¯å¾„
    zip_file = "downloaded_log.zip"
    extract_dir = os.path.splitext(zip_file)[0] + "_extracted"
    
    # ä¸‹è½½ZIPæ–‡ä»¶
    if not download_file(log_url, zip_file):
        return
    
    # è§£å‹ZIPæ–‡ä»¶
    if not extract_zip(zip_file, extract_dir):
        return
    
    # å¤„ç†è§£å‹åçš„æ–‡ä»¶
    network_data = process_extracted_files(extract_dir)
    
    # è¾“å‡ºç»“æœ
    if network_data:
        output_file = "network_analysis.json"
        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(network_data, f, ensure_ascii=False, indent=2)
        
        print(f"âœ… ç½‘ç»œæ•°æ®æå–å®Œæˆï¼Œä¿å­˜è‡³: {output_file}")
        print(f"ğŸ” å…±æå– {len(network_data)} æ¡ç½‘ç»œç›¸å…³è®°å½•")
    else:
        print("âŒ æœªæå–åˆ°ç½‘ç»œè¯·æ±‚/å“åº”ä¿¡æ¯")

if __name__ == "__main__":
    main()